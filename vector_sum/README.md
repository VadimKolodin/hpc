# Сумма элементов вектора
Лабораторная работа выполнена на c++, в среде Colab.
Код представлен в двух вариантах: 
[быстрый](https://colab.research.google.com/drive/1mf6CeM5DZmbWoVzxF1I_ZOv64X-Ly2v3?usp=sharing "Код быстрой версии на colab") и 
[медленный](https://colab.research.google.com/drive/1iTDXU3V63E8ePUZZaLDbb_RSQ67Z1N-g?usp=sharing "Код медленной версии на colab"). 
(На данный момент не могу выполнять на своей машине, покольку у меня конфликты версий CUDA и Visual Studio, 
сносить что-либо из этого пока боюсь, тк НИР выполняю на tensorflow с CUDA+cuDNN, очень шаткое равновесие версий у самого Tensorflow,
ломается от любого чиха в его сторону).

# Алгоритм
Параллельный алгоритм работает следующим образом:
1. Считается необходимое кол-во потоков - n. Это число является ближайшей степенью двойки, меньшей чем половина вектора (нужно для простоты дальнейшей редукции в пунктах 4-7).

Смотрим с перспективы потока в гриде:

2. Таким образом "половина" (чуть меньше половины, если длина вектора не степень двойки) вектора "занята" потоками (каждому элементу этой части ветора поставлен в соотвествие свой поток).
Каждый поток суммирует "свой" элемент, а также все элементы, стоящие через каждые n элементов.
3. К данному шагу каждый поток имеет промежуточную сумму своей части вектора, и если просуммировать все промежуточные суммы потоков - получится искомое число.

Смотрим с перспективы потока в блоке:

4. Каждый поток загружает свою промежуточную сумму в разделяемую память своего блока в элемент с индексом равный номеру потока в блоке. Далее ждет пока все потоки его блока сделают также.
5. После этого, первая половина потоков суммируют свою промежуточную сумму и сумму потока, номер которого больше на половину размера рабочей поверзности (в данном случае половина блока). Далее ждет остальные потоки своего блока.
6. Повторяется шаг 5, но теперь рабочая поверхность в 2 раза меньше.
7. К этому моменту остается только поток с номером в блоке  = 0, у него находится сумма элементов всех потоков своего блока.

Смотрим с перспективы потока в гриде:

8. К этому моменту в каждом блоке поток с номером 0 в своем блоке имеет промежуточную сумму промежуточных сумм всех потоков своего блока. Всего таких потоков столько, сколько блоков в гриде.
Каждый из них складывает свою промежуточную сумму в элемент выходного массива с номером равным номеру своего блока.
9. Выходной массив состоит из промоежуточных сумм, кол-во которых равно кол-ву блоков. При чем элементы лежат всплошную. Будем повторять алгоритм заново, пока в массиве не останется один элемент.


# Размеры блока и грида
Как уже было указано, количество потоков выбирается как ближайшая степень двойки, меньшая половины размера веткора. 
"Половина вектора" - чтобы каждый поток просуммирорвал хотя бы 2 элемента.
"Ближайшая степень двойки, МЕНЬШАЯ половины вектора" - если брать не меньшую, а ближайшую, то время, затрачиваемое на инициализацию потоков превышает время, сохраненное при одновременном их выполнении (проверено эмпирически).

Выбрана 1D топология блоков и их сетки, поскольку в таком случае алгоритм становится интуитивно понятным.

Размеры блока и сетки выбираются автоматически исходя из разрешенных пределов и скорости выполнения, после подсчета общего необходимого кол-ва потоков.

# Оптимизации
В первой версии парарллельной программы присутсвовали некоторые ее части, которые хотелось изменить, чтобы ускорить программу.

Первым делом было убрано выделение памяти для промежуточных входных и выходных массивов (при повторных запусках ядра), в таком случае
в выходном и входном массивах в памяти GPU находится "каша" из промежуточных сумм, на которую по большому счету все равно, тк с элементами этих массивов работаем 
в жестких рамках инлексов, лишнего ничего не берется. Это позволило ускорить работу параллельной программы  примерно в 4 раза.

Далее были сделаны следующие оптимизации:
 - более простой для подсчета алгоритм (меньше операций) для пунктов алгоритма 5 (пункт 5 и описывает более простую для подсчета версию).
 - развертка цикла для пункта 2 (поскольку мы всегда знаем отношение количества потоков и кол-ва элементов вектора, можем сказать, что один поток не просуммирует более 4х элементов).
Это позволило сократить время выполнения еще примерно на 50 %. 


# Эксперименты 
Время представлено усредненное по 36 запускам. Для экспериментов была взята оптимизированная версия программы.

Размер вектора  | Время выполнения GPU, µс 	| Время выполнения CPU, µс 
--------------- | -------------------------	| -----------------------
1024  	   		| 20     					| 4
4096  	   		| 24    					| 12
16'384  		| 28    					| 52
65'536	   		| 32    					| 205
262'144	   		| 57    					| 774
1'048'576  		| 237   					| 3386

Графики представлены в линейном и логарифмическом масштабе.

![Сравнение, линейный масштаб](https://github.com/VadimKolodin/hpc/blob/main/vector_sum/compare_linear.png?raw=true)
![Сравнение, линейный масштаб с приближением](https://github.com/VadimKolodin/hpc/blob/main/vector_sum/compare_linear_scale.png?raw=true)
![Сравнение, логарифмический масштаб](https://github.com/VadimKolodin/hpc/blob/main/vector_sum/compare_log.png?raw=true)


Из таблицы видно, что рост времени выполнения с увеличением размера для CPU - линейный, поскольку сложность алгоритма O(n).

В то время как для GPU рост времени - чуть более чем логарифмический (со скачкообразным характером), каждому потоку за каждый запуск ядра необходимо выполнять O(log(log(n))) + O(1) операций, всего запусков ядра O(log(n)).
Хорошо видно, что рост времени выполнения скачкообразный. Например для размеров вектора 1024 необходим 1 запуск ядра с 1 блоком по 1024 потока, 
а для 2048 - 2 запуска ядра, первый запуск с 4 блоками по 1024 потока, а второй запуск - один блок с 4 потоками. Поэтому разница во времени выполнения примерно равна времени запуска второго ядра (подсчет включает в себя всего 4 суммирования).
Ситуация повторяется и с размерами 16'384 (2 запуска ядра) и 65'536 (2 запсука ядра). Однако, хорошо виден скачок в 262'144, поскольку, возможно, одновременно 256 блоков запустить невозможно на данном GPU.
В ситуации с 1'048'576 элементами ситуация усугубляется, в добакок к тому, что уже нужно 3 запуска ядра.
Тем не менее, хорошо видно, что рост времени выполнения не такой большой как у CPU.

Заметим, что для размеров 1024 и 4096 время выполнения параллельного алгоритма больше, чем у последовательного,
что может быть объяснено тем, что тратится большое колчиство времени на синхронизацию потоков во время работы и их запуск.


График ускорения представлен в линейном и логарифмическом масштабе.
![Ускорение, линейный масштаб](https://github.com/VadimKolodin/hpc/blob/main/vector_sum/acceleration_linear.png?raw=true)
![Ускорение, логарифмический масштаб](https://github.com/VadimKolodin/hpc/blob/main/vector_sum/acceleration_log.png?raw=true)

График ускорения растет с размером вектора. Рост на небольших размерностях быстрый, на бОльших - резко замедляется. Связано это с резким возрастанием времени выполнения параллельной программы.
Тем не менее, далее рост ожидается скачкообразный.


Можно сделать вывод о том, что данный паралелльный алгоритм стоит применять на достаточно больших размерностях вектора. 
Наиболее высокое достигнутое ускорение - 39.7 раз для вектора с размером 2^26 элементов: время gpu = 5,385 мс; cpu = 213,798 мс (больше памяти NVIDIA Tesla T4 у Colab выделить не дает).

