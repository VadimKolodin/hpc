{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Скачиваем компилятор"
      ],
      "metadata": {
        "id": "5tTpBSwEH5v9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auZLC4RzHzbp",
        "outputId": "ad8f25cd-a411-4de4-adda-96ef3404f15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-7v7ajipr\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-7v7ajipr\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=4afcb61eb53eb3b94e1ddc6732953f0aa73170c0bb32f4002453e84d9036f1c0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k5urkpe_/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Непосредственно программа"
      ],
      "metadata": {
        "id": "aJAMmh7lH-5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <cublas_v2.h>\n",
        "#include <malloc.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <stdexcept>\n",
        "using namespace std;\n",
        "/*\n",
        " Редукция сложением массива inArray\n",
        " Ответ будет лежать в outArray[0]\n",
        "*/\n",
        "__global__ void kernel(double* inArray, double* outArray, int n) {\n",
        "\n",
        "    extern __shared__ double sharedArray[];\n",
        "\n",
        "    // считаем свой id и шаг суммирования во всем массиве\n",
        "    int grid_tid = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int grid_stride = blockDim.x * gridDim.x;\n",
        "\n",
        "    // свой id внутри блока\n",
        "    int block_tid = threadIdx.x;\n",
        "\n",
        "    // сумма своих элементов \"вне блока\"\n",
        "    double firstSum = 0;\n",
        "\n",
        "    int i = grid_tid;\n",
        "    firstSum = inArray[i] + inArray[i+grid_stride];\n",
        "    i += grid_stride<<1;\n",
        "    firstSum += i < n ? inArray[i] : 0;\n",
        "    firstSum += i+grid_stride < n ? inArray[i+grid_stride] : 0;\n",
        "\n",
        "    sharedArray[block_tid] = firstSum;\n",
        "    __syncthreads();\n",
        "    // теперь в sharedArray всплошную лежат промежуточные суммы\n",
        "\n",
        "    // теперь мы работаем только в пределах своего блока с внутренней памятью\n",
        "    int block_stride = blockDim.x>>1;\n",
        "    // будем суммировать пока не окажемся в половине sharedArray из которой берут промежуточные суммы\n",
        "    while (block_tid < block_stride) {\n",
        "        sharedArray[block_tid] += sharedArray[block_tid + block_stride];\n",
        "        //и уменьшаем шаг на 2\n",
        "        block_stride >>= 1;\n",
        "\n",
        "         __syncthreads();\n",
        "    }\n",
        "    // используем идентификатор блока (тк в каждом блоке \"до конца\" дойдет только один поток - с номером 0)\n",
        "    // чтобы в outArray итоговые-промежуточные суммы лежили всплошную\n",
        "    outArray[blockIdx.x] = sharedArray[0];\n",
        "}\n",
        "\n",
        "void count_cuda_dims(int& blocksPerGrid, int& threadsPerBlock, int n) {\n",
        "    const int MAX_THREADS_PER_BLOCK = 1024;\n",
        "\n",
        "    //сколько операций нужно выполнить в первую итерацию\n",
        "    double numOperations = (float)n / 2;\n",
        "\n",
        "    //степень двойки, при возведении в которую получим наибольшее число меньшее numOperations\n",
        "    double nearestPowLower = log2(numOperations);\n",
        "    int intPower = floor(nearestPowLower);\n",
        "    //получим опитмальное кол-во потоков (ближайшая меньшая степень двойки к кол-ву элементов/2)\n",
        "    int numThreads = 1 << intPower;\n",
        "\n",
        "    //максимум потоков в блоке - 1024\n",
        "    threadsPerBlock = numThreads < MAX_THREADS_PER_BLOCK ? numThreads : MAX_THREADS_PER_BLOCK;\n",
        "    //тут ничего округлять не надо - точно делится нацело (numThreads у нас ведь степень двойки)\n",
        "    blocksPerGrid = numThreads < MAX_THREADS_PER_BLOCK ? 1 : numThreads / threadsPerBlock;\n",
        "}\n",
        "\n",
        "void handle_cuda_result(cudaError_t cuerr, char msg[]) {\n",
        "    if (cuerr != cudaSuccess) {\n",
        "        fprintf(stderr, cudaGetErrorString(cuerr));\n",
        "        fprintf(stderr, \"\\n\");\n",
        "        throw runtime_error(msg);\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        " Загрузка данных на GPU\n",
        " @ resultGpuPointer - перезаписывается, указатель на указатель на память GPU (чтобы значение указателя сохранялось)\n",
        " @ a - указатель на массив\n",
        " @ size - размер массива\n",
        "*/\n",
        "void upload_to_device(double** resultGpuPointer, double* a, int size) {\n",
        "    int sizeInBytes = size * sizeof(double);\n",
        "    //выделяем память\n",
        "    cudaError_t cuerr = cudaMalloc((void**)resultGpuPointer, sizeInBytes);\n",
        "    handle_cuda_result(cuerr, \"Cannot allocate device array\");\n",
        "\n",
        "    // копируем массив\n",
        "    //*resultGpuPointer - разыменовываем указатель на указатель, чтобы передать указатель\n",
        "    cuerr = cudaMemcpy(*resultGpuPointer, a, sizeInBytes, cudaMemcpyHostToDevice);\n",
        "    handle_cuda_result(cuerr, \"Cannot copy a array from host to device\");\n",
        "}\n",
        "\n",
        "/*\n",
        " Выделение памяти на GPU\n",
        " @ resultGpuPointer - перезаписывается, указатель на указатель на память GPU (чтобы значение указателя сохранялось)\n",
        " @ size - размер массива\n",
        "*/\n",
        "void allocate_memory(double** resultGpuPointer,  int size) {\n",
        "    int sizeInBytes = size * sizeof(double);\n",
        "    //выделяем память\n",
        "    cudaError_t cuerr = cudaMalloc((void**)resultGpuPointer, sizeInBytes);\n",
        "    handle_cuda_result(cuerr, \"Cannot allocate device array\");\n",
        "\n",
        "}\n",
        "\n",
        "/*\n",
        "Загрузка данных на хост-машину с GPU\n",
        " @ gpuPointer - указатель на память GPU\n",
        " @ resultA - указатель на результирующий массив (память должная быть выделена)\n",
        " @ size - размер массива\n",
        "*/\n",
        "void download_from_device(double* gpuPointer, double* resultA, int size) {\n",
        "    int sizeInBytes = size * sizeof(double);\n",
        "    // копируем массив\n",
        "    cudaError_t cuerr = cudaMemcpy(resultA, gpuPointer, sizeInBytes, cudaMemcpyDeviceToHost);\n",
        "    handle_cuda_result(cuerr, \"Cannot copy a array from device to host\");\n",
        "}\n",
        "\n",
        "/*\n",
        "  Редукция сложением\n",
        "*/\n",
        "double reduce_gpu(double* inArray, double* outArray, int n){\n",
        "    cudaError_t cuerr;\n",
        "\n",
        "    int threadsPerBlock;\n",
        "    int blocksPerGrid;\n",
        "\n",
        "    count_cuda_dims(blocksPerGrid, threadsPerBlock, n);\n",
        "\n",
        "    // Выделение памяти на устройстве\n",
        "    double* inGpuPointer = NULL;\n",
        "    //передаем указатель на указатель, чтобы работать со значением указателя                                                                                                                ///\n",
        "    upload_to_device(&inGpuPointer, inArray, n);\n",
        "\n",
        "    double* outGpuPointer = NULL;\n",
        "    //нужно выделить память только под количество потоков, тк там будут лежать промежуточные суммы\n",
        "    allocate_memory(&outGpuPointer, blocksPerGrid*threadsPerBlock);\n",
        "\n",
        "    // Создание обработчиков событий\n",
        "    cudaEvent_t start, stop;\n",
        "    float gpuTime = 0.0f;\n",
        "    cuerr = cudaEventCreate(&start);\n",
        "    handle_cuda_result(cuerr, \"Cannot create CUDA start event\");\n",
        "\n",
        "    cuerr = cudaEventCreate(&stop);\n",
        "    handle_cuda_result(cuerr, \"Cannot create CUDA stop event\");\n",
        "\n",
        "    cuerr = cudaEventRecord(start, 0);\n",
        "    handle_cuda_result(cuerr, \"Cannot record CUDA start event\");\n",
        "\n",
        "\n",
        "    double* pointerBuf;\n",
        "    //Запуск ядра\n",
        "    while(n > 1) {\n",
        "        kernel <<< blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(double) >>> (inGpuPointer, outGpuPointer, n);\n",
        "        // после частичной редукции от каждого блока останется одина итоговая-промежуточная сумма\n",
        "        n = blocksPerGrid;\n",
        "        // считаем заново параметры с обновленным n\n",
        "        count_cuda_dims(blocksPerGrid, threadsPerBlock, n);\n",
        "\n",
        "        // меняем местами входной и выходной массив и не читать-писать в один и тот же, чтобы избежать dirty-read\n",
        "        // в них обоих лежит мусор на незначимых для нас местах (нам интересны только первые blocksPerGrid элементов)\n",
        "        pointerBuf = inGpuPointer;\n",
        "        inGpuPointer = outGpuPointer;\n",
        "        outGpuPointer = pointerBuf;\n",
        "    }\n",
        "\n",
        "    handle_cuda_result(cudaGetLastError(), \"Cannot launch CUDA kernel\");\n",
        "\n",
        "    // Синхронизация устройств\n",
        "    cuerr = cudaDeviceSynchronize();\n",
        "    handle_cuda_result(cudaGetLastError(), \"Cannot synchronize CUDA kernel\");\n",
        "\n",
        "    // Установка точки окончания\n",
        "    cuerr = cudaEventRecord(stop, 0);\n",
        "    handle_cuda_result(cuerr, \"Cannot record CUDA stop event\");\n",
        "\n",
        "    // Копирование результата на хост, интересует только 1 элемент, где есть результат\n",
        "    download_from_device(inGpuPointer, outArray, 1);\n",
        "\n",
        "    cuerr = cudaEventElapsedTime(&gpuTime, start, stop);\n",
        "    handle_cuda_result(cuerr, \"Cannot get elapsed time\");\n",
        "    double time = gpuTime / 1000;\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "    cudaFree(inGpuPointer);\n",
        "    cudaFree(outGpuPointer);\n",
        "    return time;\n",
        "}\n",
        "\n",
        "double reduce_cpu(double* inArray, double* outArray,  int n){\n",
        "    double sum = 0;\n",
        "    double time = clock();\n",
        "\n",
        "    for(int i = 0; i < n; ++i) {\n",
        "        sum += inArray[i];\n",
        "    }\n",
        "\n",
        "    time = clock() - time;\n",
        "    time/=CLOCKS_PER_SEC;\n",
        "\n",
        "    outArray[0] = sum;\n",
        "    return time;\n",
        "}\n",
        "\n",
        "void fill_array(double* a, int n) {\n",
        "    srand (time(NULL));\n",
        "    for(int i = 0; i < n; ++i) {\n",
        "        a[i] = rand()/1000000000.0;\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "void check(double* cpu, double* gpu,  double precision) {\n",
        "    if (fabs(cpu[0] - gpu[0]) > precision) {\n",
        "        fprintf(stderr, \"Ответы не равны: cpu=%f; gpu=%f\\n\", cpu[0], gpu[0]);\n",
        "        throw runtime_error(\"Ответы не равны\");\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    int n = 1000000;\n",
        "    double* a = (double*) malloc(n*sizeof(double));\n",
        "    double* result_gpu = (double*) malloc(sizeof(double));\n",
        "    double* result_cpu = (double*) malloc(sizeof(double));\n",
        "\n",
        "    fill_array(a, n);\n",
        "\n",
        "    double time_gpu = reduce_gpu(a,result_gpu, n);\n",
        "    printf(\"result gpu: %f, time gpu: %f\\n\",result_gpu[0], time_gpu);\n",
        "\n",
        "    double time_cpu = reduce_cpu(a, result_cpu, n);\n",
        "    printf(\"result cpu: %f, time cpu: %f\\n\", result_cpu[0], time_cpu);\n",
        "\n",
        "    check(result_cpu, result_gpu, 0.000001);\n",
        "\n",
        "    return 0;\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlNNP2zOH-Pp",
        "outputId": "61fe0058-a86a-47e3-d84b-7084b7631fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result gpu: 1072621.317247, time gpu: 0.000099\n",
            "result cpu: 1072621.317247, time cpu: 0.003187\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "n=1000000\n",
        "\n",
        "time cpu: 0.002952\n",
        "\n",
        "\n",
        "**С дебагом**\n",
        "\n",
        "time gpu: 0.003275\n",
        "\n",
        "\n",
        "**Без дебага**\n",
        "\n",
        "time gpu: 0.000638\n",
        "\n",
        "\n",
        "**Без перевыделения памяти**\n",
        "\n",
        "time gpu: 0.000152\n",
        "\n",
        "\n",
        "**Развертка первого цикла и микрооптимизаации**\n",
        "\n",
        "time gpu: 0.000099\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7STKU3qq2AN8"
      }
    }
  ]
}